{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "### 28th October 2020 by Juan-José Giraldo and Mauricio A Álvarez\n",
    "\n",
    "In this Notebook, we look at the torch library in Python that allows automatic differentiation. PyTorch will be used to implement different neural network models later on.\n",
    "\n",
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor generalises the concept of vectors and matrices to an arbitrary number of dimensions.\n",
    "\n",
    "Another name for the same concept is multidimensional arrays. The dimensionality of a tensor is the number of indexes used to refer to scalar values within the tensor. The cell below shows an example initialising a Tensor uniformly for 1D, 2D and 3D:\n",
    "\n",
    ">**Tensor** = **multidimensional arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1D presents one index with shape torch.Size([3]) :\n",
      " tensor([0.6441, 0.5082, 0.4328])\n",
      "\n",
      "Tensor 2D presents two indexes with shape torch.Size([2, 3]) :\n",
      " tensor([[0.1027, 0.3788, 0.5346],\n",
      "        [0.0007, 0.0619, 0.5751]])\n",
      "\n",
      "Tensor 3D presents three indexes with shape torch.Size([5, 2, 3]) :\n",
      " tensor([[[0.1280, 0.6951, 0.5910],\n",
      "         [0.9122, 0.5853, 0.3991]],\n",
      "\n",
      "        [[0.7031, 0.3923, 0.3849],\n",
      "         [0.9856, 0.5497, 0.9775]],\n",
      "\n",
      "        [[0.2586, 0.8982, 0.1232],\n",
      "         [0.3704, 0.8430, 0.7922]],\n",
      "\n",
      "        [[0.8782, 0.3550, 0.8269],\n",
      "         [0.8157, 0.8056, 0.9252]],\n",
      "\n",
      "        [[0.6363, 0.6206, 0.4654],\n",
      "         [0.7256, 0.1597, 0.3809]]])\n"
     ]
    }
   ],
   "source": [
    "# We first import the torch library that comes with the Anaconda distribution\n",
    "import torch \n",
    "# Tensor 1D presents 1 index\n",
    "y = torch.rand([3])\n",
    "print('Tensor 1D presents one index','with shape', y.shape,':\\n',y) #get specific size with .shape\n",
    "# Tensor 2D presents 2 indexes\n",
    "y = torch.rand([2,3])\n",
    "print('\\nTensor 2D presents two indexes','with shape',y.shape,':\\n',y)\n",
    "#Tensor 3D presents 3 indexes\n",
    "y = torch.rand([5,2,3])\n",
    "print('\\nTensor 3D presents three indexes','with shape',y.shape,':\\n',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 15.])\n",
      "tensor(23.)\n",
      "tensor(23.)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with specific values\n",
    "# NOTE: what if I do not specify a dtype?\n",
    "# the torch.float32 is the default value for dtype\n",
    "x = torch.tensor([4.0,5.0])\n",
    "y = torch.tensor([2.0,3.0],dtype=torch.float32)\n",
    "\n",
    "# print(type(x))\n",
    "# print(x.dtype)\n",
    "\n",
    "# Tensor multiplication (point-wise multiplication)\n",
    "print(x*y)\n",
    "print(y @ x)\n",
    "print(x @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise a tensor with torch.zeros or torch.ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) with shape torch.Size([3, 4]) \n",
      "\n",
      "x_ones: tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) with shape torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Rather than a list, can shape parameter be a tuple? YES.\n",
    "# x_zeros = torch.zeros([3,4])\n",
    "x_zeros = torch.zeros((3,4))\n",
    "print('x_zeros:',x_zeros,'with shape',x_zeros.shape,'\\n')\n",
    "x_ones = torch.ones([2,6])\n",
    "print('x_ones:',x_ones,'with shape',x_ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape a tensor using .view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones([3,2])\n",
    "y_reshaped = y.view(6,1)  # in contrast to the common numpy library, we use .view instead of .reshape\n",
    "print(y)\n",
    "print(y_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy interoperability \n",
    "\n",
    "PyTorch tensors can be converted efficiently to NumPy arrays and **vice versa**. By doing so, you can leverage the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `torch.numpy()`, a `torch.Tensor` object can be transformed to `numpy.ndarray` object.  \n",
    "By using `torch.from_numpy()`, a `numpy.ndarry` object can be transformed to `torch.Tensor` object.\n",
    "\n",
    "> It seems that **ALL** the methods used is from `torch` library only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array in numpy form with shape (3, 4) :\n",
      " [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "Array from Numpy to Torch with shape torch.Size([5, 8]) :\n",
      " tensor([[-1.4917,  1.9787,  0.6947, -1.2566,  1.9792,  0.0444, -1.8259,  0.6694],\n",
      "        [-0.6118, -0.5145, -0.0409,  0.1969,  0.1275,  0.1419, -0.4488,  1.6223],\n",
      "        [-0.0224,  1.4420, -1.2156, -1.5845, -0.1355,  0.1579,  1.1677, -0.4074],\n",
      "        [-0.5969, -0.1922,  0.3732,  1.0014,  0.3179, -0.8391,  0.4310, -0.3145],\n",
      "        [-0.9787, -1.2588, -1.9088,  0.2201, -0.0962,  1.1675,  0.8692,  1.9644]],\n",
      "       dtype=torch.float64)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Tensor_torch to tensor Numpy\n",
    "Tensor_torch = torch.ones(3,4)\n",
    "Tensor_numpy = Tensor_torch.numpy() #Returns a NumPy multidim. array of the right size, shape and numerical type.\n",
    "print('Array in numpy form with shape', Tensor_numpy.shape,':\\n',Tensor_numpy)\n",
    "print(type(Tensor_numpy))\n",
    "\n",
    "# Tensor Numpy to Tensor_torch\n",
    "import numpy as np\n",
    "Tensor_np = np.random.randn(5,8)    \n",
    "Tensor_numpy_to_torch = torch.from_numpy(Tensor_np)\n",
    "print('\\nArray from Numpy to Torch with shape', Tensor_numpy_to_torch.shape,':\\n',Tensor_numpy_to_torch)\n",
    "print(type(Tensor_numpy_to_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "PyTorch allows to automatically obtain the gradients of a tensor with respect to a defined function. When creating the tensor, we have to indicate that it requires the gradient computation using the flag `requires_grad`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5236, 0.9179, 0.1506], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the Tensor shows the flag `requires_grad` as True. We can also activate such a flag in a Tensor already created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0])\n",
    "print(x)\n",
    "x.requires_grad_(True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function $y=x^2+5$. The function $y$ will not only carry the result of evaluating $x$, but also the gradient function $\\frac{\\partial y}{\\partial x}$ called `grad_fn` in the new tensor $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0])\n",
    "print(x)\n",
    "\n",
    "x.requires_grad_(True)  #indicate we will need the gradients with respecto to this variable\n",
    "y = x**2 + 5\n",
    "print(type(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the partial derivative $\\frac{\\partial y}{\\partial x}$, we use the `.backward()` function and the result of the gradient evaluation is stored in `x.grad` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient: tensor([4.])\n",
      "Analitical gradient: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()  #dy/dx\n",
    "print('PyTorch gradient:', x.grad)\n",
    "\n",
    "#L et us compare with the analitical gradient of y = x**2+5\n",
    "with torch.no_grad():    #this is to only use the tensor value without its gradient information\n",
    "    dy_dx = 2*x  #analitical gradient\n",
    "print('Analitical gradient:',dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate a vector $\\mathbf{w}=[w_1, \\ldots, w_D]^{\\top}$, to compute another vector $\\mathbf{g}=[g_1, \\ldots, g_D]^{\\top}$ with elements $g_i=w_i^2+5$, then we obtain a vector $\\mathbf{g}$ that contains each evaluation of the function. If we want to obtain the gradient w.r.t $\\mathbf{w}$ by using \"g.backward()\", we have to bypass a vector of size equal to w.shape to the function, i.e., \"g.backward(vect)\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.0,2.0,3.0])\n",
    "w.requires_grad_(True)\n",
    "# print(w.grad) # It will be None\n",
    "\n",
    "g = w**2+5\n",
    "# Below, the values [1.0,1.0,1.0] are multiplied by the gradient g.backward(vect)\n",
    "# of course using the ones does not modify the value of the gradient\n",
    "# What if I do not specify a dtype? Will it be automatically set to torch.float32? YES\n",
    "# vect = torch.tensor([1.0,1.0,1.0],dtype=torch.float32) \n",
    "# vect = torch.tensor(np.ones_like(w.detach().numpy())) \n",
    "# print(vect.dtype) # torch.float32\n",
    "\n",
    "# NOTE: There is also a ones_like in troch: torch.oneslike\n",
    "g.backward(torch.from_numpy(np.ones_like(w.detach().numpy())))\n",
    "g.backward(torch.from_numpy(np.ones_like(w.detach().numpy())))\n",
    "\n",
    "# print(type(g.backward(vect))) # None type\n",
    "\n",
    "# print(id(vect))\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, when accessing the gradients in a for loop, PyTorch acummulates the gradients at each\n",
    "iteration. In order to avoid this behaviour, we have to use the function .grad.zero_() also at each iteration. See in the example below what happens when commenting and uncommenting the line \"w.grad.zero_()\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3., 12., 27.])\n",
      "tensor([ 3., 12., 27.])\n",
      "tensor([ 3., 12., 27.])\n"
     ]
    }
   ],
   "source": [
    "#Pytorch uses a cumulative process for the gradients\n",
    "w = torch.tensor([1.0,2.0,3.0])\n",
    "w.requires_grad_(True)\n",
    "\n",
    "g = w**3+5\n",
    "for i in range(3):\n",
    "    g.backward(torch.ones_like(w), retain_graph=True)\n",
    "    print(w.grad)\n",
    "\n",
    "    w.grad.zero_()    #this line avoids the acummulation of the gradients uncomment it to see its effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1\n",
    "\n",
    "Verify that the gradients provided by PyTorch coincide with the analitical gradients of the function $f = \\exp \\big(-x^2-2x- \\sin (x) \\big)$ w.r.t $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0008])\n",
      "tensor([-0.0008])\n",
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# Provide your answer here\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "f = torch.exp(- torch.pow(x, 2) - 2 * x - torch.sin(x))\n",
    "\n",
    "f.backward(torch.ones_like(x))\n",
    "\n",
    "with torch.no_grad():\n",
    "    df_dx = (-2 * x - 2 - torch.cos(x)) * torch.exp(- torch.pow(x, 2) - 2 * x - torch.sin(x))\n",
    "\n",
    "# FIXME: Is it because the different underlying algorithm that causes the different result?\n",
    "print(df_dx)\n",
    "print(x.grad)\n",
    "print(df_dx == x.grad)\n",
    "# print(df_dx[2] == x.grad[2])\n",
    "# print(df_dx[2], \"==\", x.grad[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Basic Example\n",
    "\n",
    "We now provide a very simple example of linear regression with one input dimension, $y=wx+b$, and illustrate how we use PyTorch to optimise the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificially generating data set with noises\n",
    "true_w = 1.5\n",
    "true_bias = 1.0\n",
    "Ndata = 100 \n",
    "x = torch.rand(Ndata)\n",
    "y = true_w*x + true_bias + 0.05*torch.randn(Ndata)\n",
    "\n",
    "# We make sure to set the requires_grad flag to True for both paratemers\n",
    "w = torch.tensor(0., requires_grad=True)\n",
    "bias = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two useful functions, the prediction function and the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(x, w, bias):\n",
    "    return w*x + bias\n",
    "\n",
    "\n",
    "def loss_function(y, y_pred):\n",
    "    return ((y_pred-y)**2).mean()  # Mean Squared Error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use coordinate descent to estimate the parameters of the model\n",
    "\n",
    "\\begin{align*}\n",
    "    w_{k+1} = w_k - \\eta \\frac{dE}{dw}\\\\ \n",
    "    b_{k+1} = b_k - \\eta \\frac{dE}{db}\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "We know that there is a closed form solution for $w$ and $b$ through the normal equation. The example is for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 51, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 101, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 151, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 201, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 251, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 301, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 351, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 401, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 451, Loss = 0.00198660, w = 1.501, bias = 0.994\n",
      "Iteration = 500, Loss = 0.00198660, w = 1.501, bias = 0.994\n"
     ]
    }
   ],
   "source": [
    "Max_Niter = 500\n",
    "step_size = 0.1\n",
    "for Niter in range(Max_Niter):\n",
    "    # Evaluate the prediction and the loss\n",
    "    y_approx = model_prediction(x, w, bias)\n",
    "    my_loss = loss_function(y, y_approx)\n",
    "\n",
    "    # The function .backward() has to be called in order to load the grads in w.grad\n",
    "    # Notice that here it is not necessary to bypass a vector since loss_function is a scalar function\n",
    "    # REVIEW: it is the gradient of this loss function wrt w and bias that we were talking about\n",
    "    my_loss.backward()\n",
    "\n",
    "    with torch.no_grad():        # this line avoids the gradient update while allowing to change the value of w\n",
    "        # it is necessary to avoid the grad update while modifying the variable\n",
    "        w -= step_size * w.grad\n",
    "        bias -= step_size * bias.grad\n",
    "\n",
    "    # Make the zero gradient to avoid acummulation\n",
    "    # NOTE: tensor.backward() function will add the corresponding gradient directly onto the tensor.grad object, so it is necessary to call tensor.zero_()\n",
    "    w.grad.zero_()\n",
    "    bias.grad.zero_()\n",
    "\n",
    "    # We print the loss, and the parameters values every 50 iterations\n",
    "    if Niter % 50 == 0:\n",
    "        print(\n",
    "            f'Iteration = {Niter+1}, Loss = {my_loss:.8f}, w = {w:.3f}, bias = {bias:.3f}')\n",
    "\n",
    "print(\n",
    "    f'Iteration = {Niter+1}, Loss = {my_loss:.8f}, w = {w:.3f}, bias = {bias:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12be9e6d0>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(x,y,'x')\n",
    "xtest = torch.linspace(0,1,10)\n",
    "with torch.no_grad():\n",
    "    y_pred = model_prediction(xtest,w,bias)\n",
    "plt.plot(xtest,y_pred, \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a .csv dataset\n",
    "\n",
    "We can take advantage of the interoperability between Numpy and PyTorch by loading a .csv data as a numpy array and transforming it to a Torch Tensor using `torch.from_numpy(dataset_np)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./winequality-red.csv', <http.client.HTTPMessage at 0x12beda490>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is simply to download the winequality-red.csv dataset from its root url\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', './winequality-red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.loadtxt(filepath, dtype, delimiter, skiprows)`\n",
    "\n",
    "`filepath`:  \n",
    "`dtype`:   \n",
    "`delimiter`:  \n",
    "`skiprows`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tq/g313c08177gg3qlym0wyxqn80000gq/T/ipykernel_96566/633450274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwine_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./winequality-red.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwine_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine_np\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#We take advantage of the interoperability with numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwine_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#In the line below we avoid the first row (skiprows=1) of .csv file that contains names\n",
    "#the delimeter of data for this dataset is \";\"\n",
    "wine_np = np.loadtxt(\"./winequality-red.csv\",dtype=np.float32,delimiter=\";\",skiprows=1)\n",
    "print(wine_np.shape)\n",
    "wine_torch = torch.from_numpy(wine_np)  #We take advantage of the interoperability with numpy\n",
    "\n",
    "wine_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting linear regression for the Rented Bike Dataset of lab notebook 2\n",
    "\n",
    "We will implement a linear regression for the Rented Bike dataset previously used in Lab. 2. We will use the same data preparation through `sklearn.preprocessing`: the OneHotEncoder() that allows to transform a categorical variable to a one-hot encoding representation, and StandardScaler() performs feature scaling by standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', './SeoulBikeData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was **borrowed** from Lab Notebook 2. You can go back to that Notebook for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Get/Read the data. Drop the unnecessary `date` column.\n",
    "bike_sharing_data = pd.read_csv('SeoulBikeData.csv', encoding= 'unicode_escape')\n",
    "bike_sharing_data = bike_sharing_data.drop('Date', axis=1)\n",
    "\n",
    "for col in ['Rented Bike Count', 'Hour', 'Humidity(%)', 'Visibility (10m)']:\n",
    "    bike_sharing_data[col] = bike_sharing_data[col].astype('float64')\n",
    "\n",
    "attributes_cat = ['Seasons', 'Holiday', 'Functioning Day']\n",
    "attributes_num = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
    "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\n",
    "\n",
    "# We split our dataset for Training and Testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "bs_train_set, bs_test_set = train_test_split(bike_sharing_data, test_size=0.15, random_state=42)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num),\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "])\n",
    "\n",
    "# We separate the features from the labels\n",
    "\n",
    "bs_train_set_attributes = bs_train_set.drop('Rented Bike Count', axis=1)\n",
    "bs_test_set_attributes = bs_test_set.drop('Rented Bike Count', axis=1)\n",
    "bs_train_set_labels = bs_train_set['Rented Bike Count']\n",
    "bs_test_set_labels = bs_test_set['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function `torch.from_numpy()` to transform the data previously prepared, into a Torch Tensor. We make sure to add a column of ones to the attributes (remember that $x_0=1$) both in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7446, 17])\n",
      "2\n",
      "torch.Size([7446, 18])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# We apply the preprocessing transformation over the features of the training data\n",
    "\n",
    "bs_train_set_attributes_prepared = full_transform.fit_transform(\n",
    "    bs_train_set_attributes)\n",
    "bs_test_set_attributes_prepared = full_transform.transform(\n",
    "    bs_test_set_attributes)\n",
    "\n",
    "Train_torch = torch.from_numpy(bs_train_set_attributes_prepared)\n",
    "Test_torch = torch.from_numpy(bs_test_set_attributes_prepared)\n",
    "print(Train_torch.shape)\n",
    "print(Train_torch.dim())\n",
    "\n",
    "# The line below adds a feature vector of ones in order to allow the bias weight\n",
    "# to be represented in a unique weight vector.\n",
    "\n",
    "Train_torch = torch.cat(\n",
    "    (torch.ones([Train_torch.shape[0], 1]), Train_torch), 1)\n",
    "\n",
    "print(Train_torch.shape)\n",
    "print(Train_torch.dim())\n",
    "\n",
    "# The line below adds a feature vector of ones in order to allow the bias weight\n",
    "# to be represented in a unique weight vector.\n",
    "\n",
    "# NOTE: dim = 0 means row and dim = 1 means concatenate through columns\n",
    "Test_torch = torch.cat((torch.ones([Test_torch.shape[0], 1]), Test_torch), 1)\n",
    "\n",
    "Train_Label_torch = torch.from_numpy(bs_train_set_labels.values)\n",
    "Test_Label_torch = torch.from_numpy(bs_test_set_labels.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vector of weights $\\mathbf{w}$ with the corresponding flag for the gradient and two functions, one for prediction and one for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the vector of weights to be optimised in the linear regression model\n",
    "dim = Train_torch.shape[1]\n",
    "w = torch.randn([dim,1],dtype=torch.float64)  # vector of weight w is a vector Dim x 1\n",
    "w.requires_grad_(True)\n",
    "\n",
    "# We create the model prediction which consists on an inner product X'w, where X is a design matrix of N x Dim\n",
    "def model_prediction_lr(x,w):\n",
    "    return torch.matmul(x,w)\n",
    "\n",
    "def loss_function_lr(y,y_pred):\n",
    "    return ((y_pred-y)**2).mean()  # Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally use gradient descent to find the optimal value for $\\mathbf{w}$\n",
    "$$\n",
    "\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\eta \\frac{dE(\\mathbf{w})}{d\\mathbf{w}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, Loss = 581618.15006815\n",
      "Iteration = 21, Loss = 544329.88706617\n",
      "Iteration = 41, Loss = 516852.77492370\n",
      "Iteration = 50, Loss = 506556.52455090\n"
     ]
    }
   ],
   "source": [
    "# Training the model with Gradient Descent\n",
    "\n",
    "Max_Niter = 50  # If you have many iterations, this process can take some time\n",
    "step_size = 0.001\n",
    "for Niter in range(Max_Niter):\n",
    "    y_approx = model_prediction_lr(Train_torch, w)\n",
    "    my_loss = loss_function_lr(Train_Label_torch, y_approx)\n",
    "\n",
    "    # The function .backward() has to be called in order to load the grads in w.grad\n",
    "    # Notice that here it is not necessary to bypass a vector since loss_function is a scalar function\n",
    "\n",
    "    # TODO: Need an intuitive understanding\n",
    "    my_loss.backward()\n",
    "\n",
    "    with torch.no_grad():    # this line avoids the gradient update while allowing to change the value of w\n",
    "        # it is necessary to avoid the grad update while modifying the variable\n",
    "        w -= step_size * w.grad\n",
    "\n",
    "    # print(w.grad)\n",
    "    # Make the zero gradient to avoid acummulation\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if Niter % 20 == 0 or Niter == Max_Niter-1:\n",
    "        print(f'Iteration = {Niter+1}, Loss = {my_loss:.8f}')\n",
    "        #print('Weights vector:\\n', w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally provide the RMSE for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Root Mean Squared Error over the test set is: 704.8166147886883\n"
     ]
    }
   ],
   "source": [
    "# RMSE over the test set\n",
    "\n",
    "y_pred_test = model_prediction_lr(Test_torch,w)\n",
    "MSE_test = loss_function_lr(Test_Label_torch,y_pred_test)\n",
    "print('The Root Mean Squared Error over the test set is:', np.sqrt(MSE_test.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Implement the prediction using an exponential transformation for model_prediction_lr, i.e., $y_{\\text{approx}} = \\exp(\\mathbf{w}^{\\top}\\mathbf{x})$.  \n",
    "Since the labels are positive values, this transformation guarrantees that the predictions are always positive.\n",
    ">Hint: Be careful initialising w and updating it with big step_sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your answer here\n",
    "# see details in codes/lab5.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
